{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise_1_Topic_Modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xerojester/Assignment-6/blob/main/Exercise_1_Topic_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9YDeYm34kkQ"
      },
      "source": [
        "# Exercise 1 - Topic Modeling\n",
        "\n",
        "In this notebook, we will apply our understanding of topic modeling techniques like LDA and NMF\n",
        "\n",
        "__Fill in the sections marked with `<YOUR CODE HERE>`__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av3J0QNOhMtj"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqYIRRIN4K5K"
      },
      "source": [
        "import nltk\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import gensim\n",
        "\n",
        "import re\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import NMF"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_PdhdL8h1kl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf258d31-f929-43e4-80b2-afe7af11c33a"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdFkmsXtnOLU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "939d0130-0a1d-4e10-c606-5ff53c018281"
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZLUlec3hPmu"
      },
      "source": [
        "## Get Dataset\n",
        "\n",
        "For this assignment, we will use the __20 Newsgroup__ dataset. This dataset contains ~11k news articles spread across 20 news categories. The ``sklearn`` library provides an easy to use interface to get this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6wdraJohQ85"
      },
      "source": [
        "newsgroups_train = fetch_20newsgroups(subset='train')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QGq-6r1iqs7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12c3410d-ea10-4c33-e574-f86f80c4ef0c"
      },
      "source": [
        "# view the news categories\n",
        "newsgroups_train.target_names"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV2furmyhRl5"
      },
      "source": [
        "## Pre-process Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTWRcM11sACV"
      },
      "source": [
        "## Question 1: Complete Regex to remove emails (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVgqg9v0hUw5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7740746d-11a3-4d02-8388-d2dfa266f3e3"
      },
      "source": [
        "# Convert to list\n",
        "data = newsgroups_train.data\n",
        "\n",
        "# Remove Emails\n",
        "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "\n",
        "# Remove extra spaces \\ new lines\n",
        "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "\n",
        "# Remove distracting single quotes\n",
        "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
        "\n",
        "print(data[:1])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['From: (wheres my thing) Subject: WHAT car is this!? Nntp-Posting-Host: rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: 15 I was wondering if anyone out there could enlighten me on this car I saw the other day. It was a 2-door sports car, looked to be from the late 60s/ early 70s. It was called a Bricklin. The doors were really small. In addition, the front bumper was separate from the rest of the body. This is all I know. If anyone can tellme a model name, engine specs, years of production, where this car is made, history, or whatever info you have on this funky looking car, please e-mail. Thanks, - IL ---- brought to you by your neighborhood Lerxst ---- ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKZovcQwj7yW"
      },
      "source": [
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "wnl = nltk.stem.wordnet.WordNetLemmatizer()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdwBtd1AtD0B"
      },
      "source": [
        "## Question 2: Complete the `normalize_corpus` function (2 points)\n",
        "\n",
        "__Note:__ Remove tokens with length 2 or more (as compared to 1 or more in Tutorial 1)\n",
        "\n",
        "__Hint:__ The `normalize_corpus()` function in Tutorial 1 will come in handy here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUKW-rpwkHmx"
      },
      "source": [
        "def normalize_corpus(news_articles):\n",
        "    norm_articles = []\n",
        "    for article in tqdm(news_articles):\n",
        "        article = article.lower()\n",
        "        article_tokens = [token.strip() for token in wtk.tokenize(article)]\n",
        "        article_tokens = [wnl.lemmatize(token) for token in article_tokens if not token.isnumeric()]\n",
        "        article_tokens = [token for token in article_tokens if len(token) > 1]\n",
        "        article_tokens = [token for token in article_tokens if token not in stop_words]\n",
        "        article_tokens = list(filter(None, article_tokens))\n",
        "        if article_tokens:\n",
        "            norm_articles.append(article_tokens)\n",
        "    return norm_articles"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfnwBnxnkvUL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "561719d4-55f4-4f00-a7ee-88cb6e2ee199"
      },
      "source": [
        "%%time\n",
        "\n",
        "norm_data = normalize_corpus(data)\n",
        "print(len(norm_data))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11314/11314 [00:22<00:00, 508.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11314\n",
            "CPU times: user 21.8 s, sys: 307 ms, total: 22.1 s\n",
            "Wall time: 22.2 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv-KyoGFFTnJ"
      },
      "source": [
        "# Topic Modeling with LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6JoCCPJhWue"
      },
      "source": [
        "## Feature Engineering: Bi-Grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auu8OoMBtdVi"
      },
      "source": [
        "## Question 3: Fill up the necessary code snippets to create a Bi-gram Bag of Words Model (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FBXNtXGt07Y"
      },
      "source": [
        "#### Build the bi-gram phrase model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIactpZstpN9"
      },
      "source": [
        "__Note:__ Use `min_count` and `threshold` parameters similar to the tutorial "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qsf6o2TZhYw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82886fce-3dd6-49a7-b146-d9729bfab339"
      },
      "source": [
        "bigram = gensim.models.Phrases(norm_data, \n",
        "                               min_count=20, \n",
        "                               threshold=20, \n",
        "                               delimiter=b'_')\n",
        "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
        "\n",
        "print(bigram_model[norm_data[0]][:50])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['wheres', 'thing', 'subject', 'car', 'nntp_posting', 'host', 'rac3', 'wam', 'umd_edu', 'organization_university', 'maryland_college', 'park', 'line', 'wa_wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'wa', 'door', 'sport', 'car', 'looked', 'late', '60', 'early', '70', 'wa', 'called', 'bricklin', 'door', 'really', 'small', 'addition', 'front', 'bumper', 'wa', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'spec', 'year']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_j6db-clCWJ"
      },
      "source": [
        "norm_corpus_bigrams = [bigram_model[doc] for doc in norm_data]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDLAujSIt4d4"
      },
      "source": [
        "#### Generate the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnmPEm9rlB-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc4f9df-5059-4035-8640-2437047b09b2"
      },
      "source": [
        "# Create a dictionary representation of the documents.\n",
        "dictionary = gensim.corpora.Dictionary(norm_corpus_bigrams)\n",
        "print('Sample word to number mappings:', list(dictionary.items())[:15])\n",
        "print('Total Vocabulary Size:', len(dictionary))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample word to number mappings: [(0, '60'), (1, '70'), (2, 'addition'), (3, 'anyone'), (4, 'body'), (5, 'bricklin'), (6, 'brought'), (7, 'bumper'), (8, 'called'), (9, 'car'), (10, 'could'), (11, 'day'), (12, 'door'), (13, 'early'), (14, 'engine')]\n",
            "Total Vocabulary Size: 94305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNcIM0lPt6r8"
      },
      "source": [
        "#### Remove unnecessary terms\n",
        "\n",
        "__Note:__ Use `no_below` and `no_above` parameters similar to the tutorial "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcyNn0krlThV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e634072d-b329-4e52-a300-b70dc3286b06"
      },
      "source": [
        "# Filter out words that occur less than 20 documents, \n",
        "# or more than 60% of the documents.\n",
        "dictionary.filter_extremes(no_below=20, no_above=0.6)\n",
        "print('Total Vocabulary Size:', len(dictionary))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Vocabulary Size: 7989\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKM5vpZKuHX2"
      },
      "source": [
        "#### Create the Bag of Words model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUuEMDC_ljCp"
      },
      "source": [
        "# Transforming corpus into bag of words vectors\n",
        "bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigrams]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2BQDQ5auEK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4f7aaff-6280-45d4-d48e-392ea477afc5"
      },
      "source": [
        "# view sample transformation\n",
        "print(bow_corpus[1][:50])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(10, 2), (31, 1), (42, 1), (50, 1), (51, 1), (52, 2), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 2), (59, 1), (60, 1), (61, 5), (62, 1), (63, 1), (64, 1), (65, 1), (66, 2), (67, 1), (68, 2), (69, 1), (70, 1), (71, 1), (72, 2), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 3), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 4), (95, 1), (96, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRfXyV7mhc7o"
      },
      "source": [
        "## Topic Modeling using LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfucAGie28xZ"
      },
      "source": [
        "### LDA using ``MALLET``\n",
        "The MALLET framework is a Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text. MALLET stands for __MA__chine __L__earning for __L__anguag __E__ __T__oolkit. It was developed by Andrew McCallum along with several people at the University of Massachusetts Amherst. The MALLET topic modeling toolkit contains efficient, sampling-based implementations of Latent Dirichlet Allocation, Pachinko Allocation, and Hierarchical LDA. To use MALLET’s capabilities, we need to download the framework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoHdmNUf3VjP",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81210f9a-e0bf-43e9-bd8d-07b56e52e9ed"
      },
      "source": [
        "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-07 01:21:16--  http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
            "Resolving mallet.cs.umass.edu (mallet.cs.umass.edu)... 128.119.246.70\n",
            "Connecting to mallet.cs.umass.edu (mallet.cs.umass.edu)|128.119.246.70|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16184794 (15M) [application/zip]\n",
            "Saving to: ‘mallet-2.0.8.zip’\n",
            "\n",
            "mallet-2.0.8.zip    100%[===================>]  15.43M  18.2MB/s    in 0.8s    \n",
            "\n",
            "2021-02-07 01:21:17 (18.2 MB/s) - ‘mallet-2.0.8.zip’ saved [16184794/16184794]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd_Uh0wuELGQ"
      },
      "source": [
        "!unzip -q mallet-2.0.8.zip"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuMDwKR3uY0T"
      },
      "source": [
        "## Question 4: Build an LDA topic model with MALLET (1 point)\n",
        "\n",
        "__Hint:__ Refer to the tutorial and use a similar configuration for the model settings (hyperparameters). __Also set the total topics to be 20__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcSOwOSchgRV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394f736b-55f9-42aa-cc89-5d4fc5a52f8b"
      },
      "source": [
        "%%time\n",
        "TOTAL_TOPICS = 20\n",
        "lda_model = gensim.models.LdaModel(corpus=bow_corpus, \n",
        "                                   id2word=dictionary, \n",
        "                                   chunksize=1740, \n",
        "                                   alpha='auto', \n",
        "                                   eta='auto', \n",
        "                                   random_state=42,\n",
        "                                   iterations=500, \n",
        "                                   num_topics=TOTAL_TOPICS, \n",
        "                                   passes=20, \n",
        "                                   eval_every=None)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4min, sys: 2min 10s, total: 6min 10s\n",
            "Wall time: 3min 45s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5Zl1TlU3Vgo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15bb1ece-ce47-476a-8c8f-70f562cecbc0"
      },
      "source": [
        "%%time\n",
        "\n",
        "MALLET_PATH = 'mallet-2.0.8/bin/mallet'\n",
        "lda_mallet = gensim.models.wrappers.LdaMallet(mallet_path=MALLET_PATH, \n",
        "                                              corpus=bow_corpus, \n",
        "                                              num_topics=TOTAL_TOPICS, \n",
        "                                              id2word=dictionary,\n",
        "                                              iterations=500, \n",
        "                                              workers=4)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5.83 s, sys: 63.6 ms, total: 5.89 s\n",
            "Wall time: 1min 38s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuQTfPlHunE3"
      },
      "source": [
        "__The model may take some time to run depending on your system config__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfnoqJZghhMP"
      },
      "source": [
        "## Question 5: View Topics (1 point)\n",
        "\n",
        "__Hint:__ The _View Topics_ section in Tutorial 1 might be useful here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex3iHUMBhjp0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "e8910bc6-e96a-42a9-b7b8-e074bb0b1a23"
      },
      "source": [
        "topics = [[(term, round(wt, 3)) \n",
        "               for term, wt in lda_mallet.show_topic(n, topn=20)] \n",
        "                   for n in range(0, lda_mallet.num_topics)]\n",
        "topics_df = pd.DataFrame([', '.join([term for term, wt in topic])  \n",
        "                              for topic in topics],\n",
        "                         columns = ['Terms per Topic'],\n",
        "                         index=['Topic'+str(t) for t in range(1, lda_mallet.num_topics+1)]\n",
        "                         )\n",
        "\n",
        "topics_df"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Terms per Topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic1</th>\n",
              "      <td>game, team, wa, player, year, ha, play, win, hockey, season, fan, division, point, good, time, run, hit, baseball, goal, last_year</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic2</th>\n",
              "      <td>line, mark, mike, la, canada, brian, john, blue, king, steve, thomas, van, tv, smith, keywords, organization_university, pp, tom, usa_line, wa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic3</th>\n",
              "      <td>wa, didnt, people, time, day, told, back, child, started, happened, woman, left, fire, home, armenian, hand, building, thing, made, heard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic4</th>\n",
              "      <td>question, doe, true, reason, dont, argument, claim, ha, thing, word, evidence, human, atheist, science, wrong, truth, belief, matter, point, statement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic5</th>\n",
              "      <td>dont, im, good, thing, writes, ive, make, lot, doesnt, bad, youre, isnt, problem, id, dont_know, article, wrong, give, real, ill</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic6</th>\n",
              "      <td>people, wa, law, case, make, person, ha, system, fact, action, doe, free, government, court, society, point, state, power, rule, order</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic7</th>\n",
              "      <td>year, db, center, national, research, april, program, university, report, wa, russian, office, member, general, ha, service, press, page, project, dr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic8</th>\n",
              "      <td>drive, card, system, driver, problem, mac, scsi, memory, bit, monitor, apple, pc, disk, board, work, video, machine, chip, speed, mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic9</th>\n",
              "      <td>key, space, system, encryption, data, technology, chip, ha, de, nasa, security, government, communication, clipper, bit, launch, clipper_chip, satellite, moon, earth</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic10</th>\n",
              "      <td>israel, jew, wa, israeli, people, ha, war, arab, state, land, writes, government, world, jewish, men, peace, attack, nazi, line_article, population</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic11</th>\n",
              "      <td>gun, people, ha, state, president, bill, government, job, money, weapon, american, crime, firearm, time, tax, police, clinton, make, criminal, pay</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic12</th>\n",
              "      <td>window, file, image, version, program, application, display, graphic, software, color, server, system, run, package, sun, format, motif, data, widget, screen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic13</th>\n",
              "      <td>line, posting_host, university, line_nntp, organization_university, sale, price, keywords, line_distribution, interested, distribution_usa, doe_anyone, newsreader_tin, sound, sell, mail, box, dept, phone, computer_science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic14</th>\n",
              "      <td>line, file, time, number, problem, program, point, set, read, change, entry, doe, code, bit, case, return, error, open, current, size</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic15</th>\n",
              "      <td>information, list, mail, group, internet, message, posting, address, send, book, faq, computer, post, system, service, network, article, user, email, net</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic16</th>\n",
              "      <td>ha, time, problem, food, wa, drug, work, study, effect, doctor, disease, day, level, people, health, case, medical, patient, test, msg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic17</th>\n",
              "      <td>car, power, bike, dod, light, engine, ground, back, water, road, time, mile, front, model, turn, wire, work, circuit, low, ride</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic18</th>\n",
              "      <td>god, christian, wa, jesus, people, armenian, bible, religion, book, church, life, ha, world, word, man, time, turkish, christ, greek, hell</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic19</th>\n",
              "      <td>writes, nntp_posting, host, line_article, article, line, reply, writes_article, david, reply_organization, michael, heard, jim, robert, james, wrote, inc_line, john, ca, richard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic20</th>\n",
              "      <td>ax, max, pl, sl, mr, wm, 1d9, tm, mq, m3, bj, ml, au, km, mn, ah, tg, mi, st, mw</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                       Terms per Topic\n",
              "Topic1   game, team, wa, player, year, ha, play, win, hockey, season, fan, division, point, good, time, run, hit, baseball, goal, last_year                                                                                           \n",
              "Topic2   line, mark, mike, la, canada, brian, john, blue, king, steve, thomas, van, tv, smith, keywords, organization_university, pp, tom, usa_line, wa                                                                               \n",
              "Topic3   wa, didnt, people, time, day, told, back, child, started, happened, woman, left, fire, home, armenian, hand, building, thing, made, heard                                                                                    \n",
              "Topic4   question, doe, true, reason, dont, argument, claim, ha, thing, word, evidence, human, atheist, science, wrong, truth, belief, matter, point, statement                                                                       \n",
              "Topic5   dont, im, good, thing, writes, ive, make, lot, doesnt, bad, youre, isnt, problem, id, dont_know, article, wrong, give, real, ill                                                                                             \n",
              "Topic6   people, wa, law, case, make, person, ha, system, fact, action, doe, free, government, court, society, point, state, power, rule, order                                                                                       \n",
              "Topic7   year, db, center, national, research, april, program, university, report, wa, russian, office, member, general, ha, service, press, page, project, dr                                                                        \n",
              "Topic8   drive, card, system, driver, problem, mac, scsi, memory, bit, monitor, apple, pc, disk, board, work, video, machine, chip, speed, mode                                                                                       \n",
              "Topic9   key, space, system, encryption, data, technology, chip, ha, de, nasa, security, government, communication, clipper, bit, launch, clipper_chip, satellite, moon, earth                                                        \n",
              "Topic10  israel, jew, wa, israeli, people, ha, war, arab, state, land, writes, government, world, jewish, men, peace, attack, nazi, line_article, population                                                                          \n",
              "Topic11  gun, people, ha, state, president, bill, government, job, money, weapon, american, crime, firearm, time, tax, police, clinton, make, criminal, pay                                                                           \n",
              "Topic12  window, file, image, version, program, application, display, graphic, software, color, server, system, run, package, sun, format, motif, data, widget, screen                                                                \n",
              "Topic13  line, posting_host, university, line_nntp, organization_university, sale, price, keywords, line_distribution, interested, distribution_usa, doe_anyone, newsreader_tin, sound, sell, mail, box, dept, phone, computer_science\n",
              "Topic14  line, file, time, number, problem, program, point, set, read, change, entry, doe, code, bit, case, return, error, open, current, size                                                                                        \n",
              "Topic15  information, list, mail, group, internet, message, posting, address, send, book, faq, computer, post, system, service, network, article, user, email, net                                                                    \n",
              "Topic16  ha, time, problem, food, wa, drug, work, study, effect, doctor, disease, day, level, people, health, case, medical, patient, test, msg                                                                                       \n",
              "Topic17  car, power, bike, dod, light, engine, ground, back, water, road, time, mile, front, model, turn, wire, work, circuit, low, ride                                                                                              \n",
              "Topic18  god, christian, wa, jesus, people, armenian, bible, religion, book, church, life, ha, world, word, man, time, turkish, christ, greek, hell                                                                                   \n",
              "Topic19  writes, nntp_posting, host, line_article, article, line, reply, writes_article, david, reply_organization, michael, heard, jim, robert, james, wrote, inc_line, john, ca, richard                                            \n",
              "Topic20  ax, max, pl, sl, mr, wm, 1d9, tm, mq, m3, bj, ml, au, km, mn, ah, tg, mi, st, mw                                                                                                                                             "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK-HZ6fOhmfu"
      },
      "source": [
        "## Question 6: Evaluate Model Performance (1 point)\n",
        "\n",
        "__Note:__ print the Cv and UMass coherence scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwFhvbQ6honk"
      },
      "source": [
        "cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n",
        "                                                      texts=norm_corpus_bigrams,\n",
        "                                                      dictionary=dictionary, \n",
        "                                                      coherence='c_v')\n",
        "\n",
        "avg_coherence_cv = cv_coherence_model_lda.get_coherence()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHZwyaeN6ZZr"
      },
      "source": [
        "umass_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n",
        "                                                         texts=norm_corpus_bigrams,\n",
        "                                                         dictionary=dictionary, \n",
        "                                                         coherence='u_mass')\n",
        "\n",
        "avg_coherence_umass = umass_coherence_model_lda.get_coherence()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gHio7177XV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fa1f192-3d19-44b3-bc0b-edd97ab1d152"
      },
      "source": [
        "print('Avg. Coherence Score (Cv):', avg_coherence_cv)\n",
        "print('Avg. Coherence Score (UMass):', avg_coherence_umass)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avg. Coherence Score (Cv): 0.6003681369400063\n",
            "Avg. Coherence Score (UMass): -2.471784412585273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF4WlQ8V4qfY"
      },
      "source": [
        "## Inference on documents\n",
        "\n",
        "Here we will try to take some documents and predict \\ infer their topics using our trained LDA model. Do note you can use any new documents also in this scenario but you would need to transform them into relevant bag of words vectors before predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTj2l97F-PbB"
      },
      "source": [
        "#### Create a sample dataset of 3 documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyyIBAq_oBCy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d29451f1-5d14-49ee-a5e2-4d729f7537fa"
      },
      "source": [
        "sample_docs = [' '.join(doc) for doc in norm_data[5:8]]\n",
        "sample_docs"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['foxvog douglas subject rewording second amendment idea organization vtt line article tavares writes article foxvog douglas writes article tavares writes article john lawrence rutledge writes massive destructive power many modern weapon make cost accidental crimial usage weapon great weapon mass destruction need control government individual access would result needle death million make right people keep bear many modern weapon non existant thanks stating youre coming needle say disagree every count believe individual right weapon mass destruction find hard believe would support neighbor right keep nuclear weapon biological weapon nerve gas property cannot even agree keeping weapon mass destruction hand individual hope dont sign blank check course term must rigidly defined bill doug foxvog say weapon mass destruction mean cbw nuke sarah brady say weapon mass destruction mean street sweeper shotgun semi automatic sks rifle doubt us term using quote allegedly back john lawrence rutledge say weapon mass destruction immediately follows ha thousand people killed year handgun number easily reduced putting reasonable restriction doe rutledge mean term read article presenting first argument weapon mass destruction commonly understood switching topic first point evidently wa show weapon allowed later analysis wa given understanding consider another class believe speak company write today special investor packet doug foxvog',\n",
              " 'brian manning delaney subject brain tumor treatment thanks reply organization university chicago line people responded request info treatment astrocytomas email couldnt thank directly mail bouncing probs sean debra sharon thought id publicly thank everyone thanks im sure glad accidentally hit rn instead rm wa trying delete file last september hmmm news whats brian',\n",
              " 'grubb subject ide scsi organization new mexico state university la crux nm line distribution world nntp posting host dante nmsu edu writes article grubb say pc magazine april although scsi twice fasst esdi faster ide support device acceptance ha long stalled incompatability problem installation headache love magazine writer make stupid statement like performance get number ill list actual performance range convince anyone statement absurd scsi range 5mb scsi ii range 40mb ide range 3mb esdi always 25mb although non standard version show dont know much scsi scsi scsi controler chip range indeed 5mb right scsi scsi scsi controller chip 6mb 10mb burst bit note increase speed mac quadra us version scsi doe exist pc use set scsi bit scsi mode 6mb 10mb burst scsi bit wide fast mode 12mb 20mb burst scsi bit wide fast 20mb 40mb burst data although scsi twice fast esdi correct scsi controller chip scsi reach 10mb indeed faster ide scsi fact posted newsgroup mac ibm info sheet available ftp sumex aim stanford edu info mac report mac ibm compare version txt may still part problem mac ibm pc inconsiant scsi though well documented quadra ha scsi chip apple salesperson said us fast scsi chip 6mb 10mb burst doe scsi 5mb maximum synchronous quadra us ansynchronous scsi slower seems mac ibm see scsi interface think scsi maybe scsi interface driven machine scsi controller chip bit mode much faster true scsi go dont slam article dont understand going one reference quadras scsi controller chip digital review oct v8 n33 p8']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRErT61S-S-O"
      },
      "source": [
        "#### Check their class labels\n",
        "\n",
        "Since this is actually a labeled dataset we can see the actual class \\ category labels of these news posts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN36ZXGAoihS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f66dadb-1b18-420f-cbac-dc91f7c76684"
      },
      "source": [
        "print(np.array(newsgroups_train.target_names)[newsgroups_train.target[5:8]])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['talk.politics.guns' 'sci.med' 'comp.sys.ibm.pc.hardware']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snF0CRNI-cTJ"
      },
      "source": [
        "## Question 7: Pre-process documents (1 point)\n",
        "\n",
        "__Note:__ You can refer to Tutorial 1 or even refer to the steps above (before building them model)\n",
        "\n",
        "1. Tokenize the sample documents to get list of words per document (string splitting is useful here)\n",
        "\n",
        "2. Get bigram phrases for each tokenized document using `bigram_model`\n",
        "\n",
        "3. Use the `dictionary` built previously in the above section to get the BOW vectors using `gensim`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIjdfQXHoxj_"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "# 1. Tokenize documents\n",
        "tokenized_norm_docs = [word_tokenize(doc) for doc in sample_docs]\n",
        "\n",
        "# 2. Bi-gram phrases for tokenized documents\n",
        "bigram_data = [bigram_model[doc] for doc in tokenized_norm_docs]\n",
        "\n",
        "# 3. BOW vectors for each document\n",
        "bow_vectorized_features = [dictionary.doc2bow(text) for text in bigram_data]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7bCPG3r_Deg"
      },
      "source": [
        "## Question 8: Inference with trained topic model (1 point)\n",
        "\n",
        "__Note:__ Use the trained `lda_mallet` model from above to predict and get the top (most dominant) topic per document. Remember to refer to the __Interpret Results__ section in Tutorial 1 if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZUY7WaypLOy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "90514bfa-73ef-494e-f254-65002ca09edd"
      },
      "source": [
        "predicted_topics = \n",
        "top_topics = \n",
        "                     \n",
        "final_topics = [(topic+1, weight) for topic, weight in top_topics]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-662fc008d71a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtop_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredicted_topics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfinal_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_topics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-662fc008d71a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtop_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredicted_topics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfinal_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_topics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLJUPY8Lpj3q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "a08dec84-bb97-4b99-82cd-ddcb4cb110f8"
      },
      "source": [
        "print(final_topics)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-3719fd0e9143>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'final_topics' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sld3dORTqbWN"
      },
      "source": [
        "[topics_df.loc['Topic'+str(topic_id)]['Terms per Topic'] \n",
        "    for topic_id, weight in final_topics]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR8klT0Nm9kG"
      },
      "source": [
        "# Topic Modeling using NMF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeepPn1mmlzl"
      },
      "source": [
        "## Get list of documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrflzNPN7wDT"
      },
      "source": [
        "norm_docs = [' '.join(tokenized_doc) for tokenized_doc in norm_data]"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rCRXxSuCpZe"
      },
      "source": [
        "## Question 9: Generate Bag of Words features (1 point)\n",
        "\n",
        "__Note:__\n",
        "\n",
        "1. Use `CountVectorizer` \n",
        "2. Set `min_df` as 20 and `max_df` as 0.6\n",
        "3. Use both 1 and 2-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlHqU5gkmKZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f701ce-3bff-4d5a-db6b-50a01ab0453e"
      },
      "source": [
        "cv = CountVectorizer(min_df=20, max_df=0.6, ngram_range=(1,2),\n",
        "                     token_pattern=None, tokenizer=lambda doc: doc,\n",
        "                     preprocessor=lambda doc: doc)\n",
        "cv_features = cv.fit_transform(norm_docs)\n",
        "\n",
        "cv_features.shape"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 1164)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPjVLMaUm1Sn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6625dd5-4f2b-4b73-973a-8a798a678544"
      },
      "source": [
        "vocabulary = np.array(cv.get_feature_names())\n",
        "print('Total Vocabulary Size:', len(vocabulary))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Vocabulary Size: 1164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHjVQ3PbGU8y"
      },
      "source": [
        "## Question 10: Train NMF Topic Model (1 point)\n",
        "\n",
        "__Note:__ You can use a similar config as Tutorial 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hujiMlutm5ay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d110d0-2588-474e-8436-5daa43b412d9"
      },
      "source": [
        "%%time \n",
        "\n",
        "nmf_model = NMF(n_components=TOTAL_TOPICS, solver='cd', max_iter=500,\n",
        "                random_state=42, alpha=.1, l1_ratio=.85)\n",
        "document_topics = nmf_model.fit_transform(cv_features)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 23.9 s, sys: 19.1 s, total: 43 s\n",
            "Wall time: 21.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aNwiDogGtrC"
      },
      "source": [
        "## Question 11: Display Topics and their Terms (2 points)\n",
        "\n",
        "__Note:__ We have done a similar exercise in Tutorial 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QhBWPnLm7fi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "843db1cb-7674-4d6b-ab48-adc5f2029b4f"
      },
      "source": [
        "topic_terms = nmf_model.components_\r\n",
        "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\r\n",
        "topic_keyterms = vocabulary[topic_key_term_idxs]\r\n",
        "topics = [', '.join(topic) for topic in topic_keyterms]\r\n",
        "pd.set_option('display.max_colwidth', -1)\r\n",
        "topics_df = pd.DataFrame(topics,\r\n",
        "                         columns = ['Terms per Topic'],\r\n",
        "                         index=['Topic'+str(t) for t in range(1, TOTAL_TOPICS+1)])\r\n",
        "topics_df"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-4f4ad70b4910>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtopic_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtopic_key_term_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mtop_terms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtopic_keyterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_key_term_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic_keyterms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_colwidth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'top_terms' is not defined"
          ]
        }
      ]
    }
  ]
}