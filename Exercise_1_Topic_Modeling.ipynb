{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9YDeYm34kkQ"
   },
   "source": [
    "# Exercise 1 - Topic Modeling\n",
    "\n",
    "In this notebook, we will apply our understanding of topic modeling techniques like LDA and NMF\n",
    "\n",
    "__Fill in the sections marked with `<YOUR CODE HERE>`__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Av3J0QNOhMtj"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yqYIRRIN4K5K"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "\n",
    "import re\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "h_PdhdL8h1kl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\xeroj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\xeroj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\xeroj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gdFkmsXtnOLU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-0891b765a168>:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZLUlec3hPmu"
   },
   "source": [
    "## Get Dataset\n",
    "\n",
    "For this assignment, we will use the __20 Newsgroup__ dataset. This dataset contains ~11k news articles spread across 20 news categories. The ``sklearn`` library provides an easy to use interface to get this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o6wdraJohQ85"
   },
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-QGq-6r1iqs7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the news categories\n",
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iV2furmyhRl5"
   },
   "source": [
    "## Pre-process Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTWRcM11sACV"
   },
   "source": [
    "## Question 1: Complete Regex to remove emails (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cVgqg9v0hUw5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: (wheres my thing) Subject: WHAT car is this!? Nntp-Posting-Host: rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: 15 I was wondering if anyone out there could enlighten me on this car I saw the other day. It was a 2-door sports car, looked to be from the late 60s/ early 70s. It was called a Bricklin. The doors were really small. In addition, the front bumper was separate from the rest of the body. This is all I know. If anyone can tellme a model name, engine specs, years of production, where this car is made, history, or whatever info you have on this funky looking car, please e-mail. Thanks, - IL ---- brought to you by your neighborhood Lerxst ---- ']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = newsgroups_train.data\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove extra spaces \\ new lines\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "print(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UKZovcQwj7yW"
   },
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdwBtd1AtD0B"
   },
   "source": [
    "## Question 2: Complete the `normalize_corpus` function (2 points)\n",
    "\n",
    "__Note:__ Remove tokens with length 2 or more (as compared to 1 or more in Tutorial 1)\n",
    "\n",
    "__Hint:__ The `normalize_corpus()` function in Tutorial 1 will come in handy here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lUKW-rpwkHmx"
   },
   "outputs": [],
   "source": [
    "def normalize_corpus(news_articles):\n",
    "    norm_articles = []\n",
    "    for article in tqdm(news_articles):\n",
    "        article = article.lower()\n",
    "        article_tokens = [token.strip() for token in wtk.tokenize(article)]\n",
    "        article_tokens = [wnl.lemmatize(token) for token in article_tokens if not token.isnumeric()]\n",
    "        article_tokens = [token for token in article_tokens if len(token) > 1]\n",
    "        article_tokens = [token for token in article_tokens if token not in stop_words]\n",
    "        article_tokens = list(filter(None, article_tokens))\n",
    "        if article_tokens:\n",
    "            norm_articles.append(article_tokens)\n",
    "    return norm_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gfnwBnxnkvUL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11314/11314 [00:32<00:00, 343.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "Wall time: 32.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "norm_data = normalize_corpus(data)\n",
    "print(len(norm_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qv-KyoGFFTnJ"
   },
   "source": [
    "# Topic Modeling with LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6JoCCPJhWue"
   },
   "source": [
    "## Feature Engineering: Bi-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auu8OoMBtdVi"
   },
   "source": [
    "## Question 3: Fill up the necessary code snippets to create a Bi-gram Bag of Words Model (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FBXNtXGt07Y"
   },
   "source": [
    "#### Build the bi-gram phrase model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIactpZstpN9"
   },
   "source": [
    "__Note:__ Use `min_count` and `threshold` parameters similar to the tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Qsf6o2TZhYw_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wheres', 'thing', 'subject', 'car', 'nntp_posting', 'host', 'rac3', 'wam', 'umd_edu', 'organization_university', 'maryland_college', 'park', 'line', 'wa_wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'wa', 'door', 'sport', 'car', 'looked', 'late', '60', 'early', '70', 'wa', 'called', 'bricklin', 'door', 'really', 'small', 'addition', 'front', 'bumper', 'wa', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'spec', 'year']\n"
     ]
    }
   ],
   "source": [
    "bigram = gensim.models.Phrases(norm_data, \n",
    "                               min_count=20, \n",
    "                               threshold=20, \n",
    "                               delimiter=b'_')\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "print(bigram_model[norm_data[0]][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "k_j6db-clCWJ"
   },
   "outputs": [],
   "source": [
    "norm_corpus_bigrams = [bigram_model[doc] for doc in norm_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDLAujSIt4d4"
   },
   "source": [
    "#### Generate the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XnmPEm9rlB-Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample word to number mappings: [(0, '60'), (1, '70'), (2, 'addition'), (3, 'anyone'), (4, 'body'), (5, 'bricklin'), (6, 'brought'), (7, 'bumper'), (8, 'called'), (9, 'car'), (10, 'could'), (11, 'day'), (12, 'door'), (13, 'early'), (14, 'engine')]\n",
      "Total Vocabulary Size: 94305\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary representation of the documents.\n",
    "dictionary = gensim.corpora.Dictionary(norm_corpus_bigrams)\n",
    "print('Sample word to number mappings:', list(dictionary.items())[:15])\n",
    "print('Total Vocabulary Size:', len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNcIM0lPt6r8"
   },
   "source": [
    "#### Remove unnecessary terms\n",
    "\n",
    "__Note:__ Use `no_below` and `no_above` parameters similar to the tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IcyNn0krlThV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 7989\n"
     ]
    }
   ],
   "source": [
    "# Filter out words that occur less than 20 documents, \n",
    "# or more than 60% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.6)\n",
    "print('Total Vocabulary Size:', len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKM5vpZKuHX2"
   },
   "source": [
    "#### Create the Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WUuEMDC_ljCp"
   },
   "outputs": [],
   "source": [
    "# Transforming corpus into bag of words vectors\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "C2BQDQ5auEK_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10, 2), (31, 1), (42, 1), (50, 1), (51, 1), (52, 2), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 2), (59, 1), (60, 1), (61, 5), (62, 1), (63, 1), (64, 1), (65, 1), (66, 2), (67, 1), (68, 2), (69, 1), (70, 1), (71, 1), (72, 2), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 3), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 4), (95, 1), (96, 1)]\n"
     ]
    }
   ],
   "source": [
    "# view sample transformation\n",
    "print(bow_corpus[1][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRfXyV7mhc7o"
   },
   "source": [
    "## Topic Modeling using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfucAGie28xZ"
   },
   "source": [
    "### LDA using ``MALLET``\n",
    "The MALLET framework is a Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text. MALLET stands for __MA__chine __L__earning for __L__anguag __E__ __T__oolkit. It was developed by Andrew McCallum along with several people at the University of Massachusetts Amherst. The MALLET topic modeling toolkit contains efficient, sampling-based implementations of Latent Dirichlet Allocation, Pachinko Allocation, and Hierarchical LDA. To use MALLET’s capabilities, we need to download the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "RoHdmNUf3VjP",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2021-02-06 00:35:47--  http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
      "Resolving mallet.cs.umass.edu (mallet.cs.umass.edu)... 128.119.246.70\n",
      "Connecting to mallet.cs.umass.edu (mallet.cs.umass.edu)|128.119.246.70|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16184794 (15M) [application/zip]\n",
      "Saving to: 'mallet-2.0.8.zip.4'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0%  554K 28s\n",
      "    50K .......... .......... .......... .......... ..........  0% 1.15M 21s\n",
      "   100K .......... .......... .......... .......... ..........  0% 3.96M 15s\n",
      "   150K .......... .......... .......... .......... ..........  1% 4.89M 12s\n",
      "   200K .......... .......... .......... .......... ..........  1% 1.48M 12s\n",
      "   250K .......... .......... .......... .......... ..........  1% 4.69M 10s\n",
      "   300K .......... .......... .......... .......... ..........  2% 1.76M 10s\n",
      "   350K .......... .......... .......... .......... ..........  2% 2.52M 9s\n",
      "   400K .......... .......... .......... .......... ..........  2% 6.10M 9s\n",
      "   450K .......... .......... .......... .......... ..........  3% 5.94M 8s\n",
      "   500K .......... .......... .......... .......... ..........  3% 16.0M 7s\n",
      "   550K .......... .......... .......... .......... ..........  3% 12.0M 7s\n",
      "   600K .......... .......... .......... .......... ..........  4% 2.88M 7s\n",
      "   650K .......... .......... .......... .......... ..........  4% 10.8M 6s\n",
      "   700K .......... .......... .......... .......... ..........  4% 4.34M 6s\n",
      "   750K .......... .......... .......... .......... ..........  5% 42.1M 6s\n",
      "   800K .......... .......... .......... .......... ..........  5% 4.22M 6s\n",
      "   850K .......... .......... .......... .......... ..........  5% 20.2M 5s\n",
      "   900K .......... .......... .......... .......... ..........  6% 21.9M 5s\n",
      "   950K .......... .......... .......... .......... ..........  6% 30.0M 5s\n",
      "  1000K .......... .......... .......... .......... ..........  6% 37.1M 5s\n",
      "  1050K .......... .......... .......... .......... ..........  6% 24.4M 4s\n",
      "  1100K .......... .......... .......... .......... ..........  7% 5.84M 4s\n",
      "  1150K .......... .......... .......... .......... ..........  7% 10.7M 4s\n",
      "  1200K .......... .......... .......... .......... ..........  7% 8.49M 4s\n",
      "  1250K .......... .......... .......... .......... ..........  8% 11.9M 4s\n",
      "  1300K .......... .......... .......... .......... ..........  8% 6.58M 4s\n",
      "  1350K .......... .......... .......... .......... ..........  8% 7.48M 4s\n",
      "  1400K .......... .......... .......... .......... ..........  9% 4.95M 4s\n",
      "  1450K .......... .......... .......... .......... ..........  9% 13.7M 4s\n",
      "  1500K .......... .......... .......... .......... ..........  9% 23.0M 3s\n",
      "  1550K .......... .......... .......... .......... .......... 10% 14.0M 3s\n",
      "  1600K .......... .......... .......... .......... .......... 10% 14.7M 3s\n",
      "  1650K .......... .......... .......... .......... .......... 10% 12.8M 3s\n",
      "  1700K .......... .......... .......... .......... .......... 11% 32.4M 3s\n",
      "  1750K .......... .......... .......... .......... .......... 11% 18.1M 3s\n",
      "  1800K .......... .......... .......... .......... .......... 11% 9.99M 3s\n",
      "  1850K .......... .......... .......... .......... .......... 12% 7.21M 3s\n",
      "  1900K .......... .......... .......... .......... .......... 12% 10.7M 3s\n",
      "  1950K .......... .......... .......... .......... .......... 12% 6.02M 3s\n",
      "  2000K .......... .......... .......... .......... .......... 12% 4.89M 3s\n",
      "  2050K .......... .......... .......... .......... .......... 13% 13.0M 3s\n",
      "  2100K .......... .......... .......... .......... .......... 13% 12.4M 3s\n",
      "  2150K .......... .......... .......... .......... .......... 13% 12.7M 3s\n",
      "  2200K .......... .......... .......... .......... .......... 14% 28.9M 3s\n",
      "  2250K .......... .......... .......... .......... .......... 14% 25.2M 3s\n",
      "  2300K .......... .......... .......... .......... .......... 14% 14.5M 3s\n",
      "  2350K .......... .......... .......... .......... .......... 15% 10.4M 3s\n",
      "  2400K .......... .......... .......... .......... .......... 15% 16.5M 2s\n",
      "  2450K .......... .......... .......... .......... .......... 15% 9.72M 2s\n",
      "  2500K .......... .......... .......... .......... .......... 16% 13.7M 2s\n",
      "  2550K .......... .......... .......... .......... .......... 16% 11.0M 2s\n",
      "  2600K .......... .......... .......... .......... .......... 16% 4.77M 2s\n",
      "  2650K .......... .......... .......... .......... .......... 17% 10.3M 2s\n",
      "  2700K .......... .......... .......... .......... .......... 17% 15.0M 2s\n",
      "  2750K .......... .......... .......... .......... .......... 17% 10.3M 2s\n",
      "  2800K .......... .......... .......... .......... .......... 18% 10.9M 2s\n",
      "  2850K .......... .......... .......... .......... .......... 18% 13.5M 2s\n",
      "  2900K .......... .......... .......... .......... .......... 18% 12.1M 2s\n",
      "  2950K .......... .......... .......... .......... .......... 18% 12.3M 2s\n",
      "  3000K .......... .......... .......... .......... .......... 19% 10.1M 2s\n",
      "  3050K .......... .......... .......... .......... .......... 19% 52.9M 2s\n",
      "  3100K .......... .......... .......... .......... .......... 19% 30.0M 2s\n",
      "  3150K .......... .......... .......... .......... .......... 20% 17.7M 2s\n",
      "  3200K .......... .......... .......... .......... .......... 20% 8.35M 2s\n",
      "  3250K .......... .......... .......... .......... .......... 20% 6.66M 2s\n",
      "  3300K .......... .......... .......... .......... .......... 21% 10.1M 2s\n",
      "  3350K .......... .......... .......... .......... .......... 21% 10.5M 2s\n",
      "  3400K .......... .......... .......... .......... .......... 21% 11.8M 2s\n",
      "  3450K .......... .......... .......... .......... .......... 22% 8.48M 2s\n",
      "  3500K .......... .......... .......... .......... .......... 22% 10.9M 2s\n",
      "  3550K .......... .......... .......... .......... .......... 22% 14.0M 2s\n",
      "  3600K .......... .......... .......... .......... .......... 23% 12.1M 2s\n",
      "  3650K .......... .......... .......... .......... .......... 23% 35.8M 2s\n",
      "  3700K .......... .......... .......... .......... .......... 23% 29.4M 2s\n",
      "  3750K .......... .......... .......... .......... .......... 24% 22.4M 2s\n",
      "  3800K .......... .......... .......... .......... .......... 24% 33.2M 2s\n",
      "  3850K .......... .......... .......... .......... .......... 24% 4.99M 2s\n",
      "  3900K .......... .......... .......... .......... .......... 24% 12.5M 2s\n",
      "  3950K .......... .......... .......... .......... .......... 25% 8.13M 2s\n",
      "  4000K .......... .......... .......... .......... .......... 25% 14.0M 2s\n",
      "  4050K .......... .......... .......... .......... .......... 25% 10.7M 2s\n",
      "  4100K .......... .......... .......... .......... .......... 26% 10.3M 2s\n",
      "  4150K .......... .......... .......... .......... .......... 26% 8.92M 2s\n",
      "  4200K .......... .......... .......... .......... .......... 26% 11.8M 2s\n",
      "  4250K .......... .......... .......... .......... .......... 27% 13.1M 2s\n",
      "  4300K .......... .......... .......... .......... .......... 27% 72.7M 2s\n",
      "  4350K .......... .......... .......... .......... .......... 27% 22.8M 2s\n",
      "  4400K .......... .......... .......... .......... .......... 28% 28.5M 2s\n",
      "  4450K .......... .......... .......... .......... .......... 28% 27.7M 2s\n",
      "  4500K .......... .......... .......... .......... .......... 28% 13.2M 2s\n",
      "  4550K .......... .......... .......... .......... .......... 29% 6.11M 2s\n",
      "  4600K .......... .......... .......... .......... .......... 29% 7.85M 2s\n",
      "  4650K .......... .......... .......... .......... .......... 29% 10.2M 2s\n",
      "  4700K .......... .......... .......... .......... .......... 30% 16.4M 1s\n",
      "  4750K .......... .......... .......... .......... .......... 30% 11.2M 1s\n",
      "  4800K .......... .......... .......... .......... .......... 30% 11.7M 1s\n",
      "  4850K .......... .......... .......... .......... .......... 31% 12.2M 1s\n",
      "  4900K .......... .......... .......... .......... .......... 31% 16.5M 1s\n",
      "  4950K .......... .......... .......... .......... .......... 31% 25.7M 1s\n",
      "  5000K .......... .......... .......... .......... .......... 31% 16.6M 1s\n",
      "  5050K .......... .......... .......... .......... .......... 32% 35.0M 1s\n",
      "  5100K .......... .......... .......... .......... .......... 32% 18.5M 1s\n",
      "  5150K .......... .......... .......... .......... .......... 32% 43.7M 1s\n",
      "  5200K .......... .......... .......... .......... .......... 33% 12.5M 1s\n",
      "  5250K .......... .......... .......... .......... .......... 33% 6.12M 1s\n",
      "  5300K .......... .......... .......... .......... .......... 33% 8.88M 1s\n",
      "  5350K .......... .......... .......... .......... .......... 34% 7.64M 1s\n",
      "  5400K .......... .......... .......... .......... .......... 34% 11.5M 1s\n",
      "  5450K .......... .......... .......... .......... .......... 34% 14.2M 1s\n",
      "  5500K .......... .......... .......... .......... .......... 35% 12.4M 1s\n",
      "  5550K .......... .......... .......... .......... .......... 35% 9.39M 1s\n",
      "  5600K .......... .......... .......... .......... .......... 35% 13.3M 1s\n",
      "  5650K .......... .......... .......... .......... .......... 36% 18.4M 1s\n",
      "  5700K .......... .......... .......... .......... .......... 36% 34.5M 1s\n",
      "  5750K .......... .......... .......... .......... .......... 36% 23.1M 1s\n",
      "  5800K .......... .......... .......... .......... .......... 37% 17.0M 1s\n",
      "  5850K .......... .......... .......... .......... .......... 37% 12.6M 1s\n",
      "  5900K .......... .......... .......... .......... .......... 37%  151M 1s\n",
      "  5950K .......... .......... .......... .......... .......... 37% 11.6M 1s\n",
      "  6000K .......... .......... .......... .......... .......... 38% 9.20M 1s\n",
      "  6050K .......... .......... .......... .......... .......... 38% 9.58M 1s\n",
      "  6100K .......... .......... .......... .......... .......... 38% 7.44M 1s\n",
      "  6150K .......... .......... .......... .......... .......... 39% 14.1M 1s\n",
      "  6200K .......... .......... .......... .......... .......... 39% 12.5M 1s\n",
      "  6250K .......... .......... .......... .......... .......... 39% 9.53M 1s\n",
      "  6300K .......... .......... .......... .......... .......... 40% 15.5M 1s\n",
      "  6350K .......... .......... .......... .......... .......... 40% 13.1M 1s\n",
      "  6400K .......... .......... .......... .......... .......... 40% 22.8M 1s\n",
      "  6450K .......... .......... .......... .......... .......... 41% 26.0M 1s\n",
      "  6500K .......... .......... .......... .......... .......... 41% 23.7M 1s\n",
      "  6550K .......... .......... .......... .......... .......... 41% 25.9M 1s\n",
      "  6600K .......... .......... .......... .......... .......... 42% 31.7M 1s\n",
      "  6650K .......... .......... .......... .......... .......... 42% 13.1M 1s\n",
      "  6700K .......... .......... .......... .......... .......... 42% 9.79M 1s\n",
      "  6750K .......... .......... .......... .......... .......... 43% 9.61M 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6800K .......... .......... .......... .......... .......... 43% 11.5M 1s\n",
      "  6850K .......... .......... .......... .......... .......... 43% 11.6M 1s\n",
      "  6900K .......... .......... .......... .......... .......... 43% 10.4M 1s\n",
      "  6950K .......... .......... .......... .......... .......... 44% 7.78M 1s\n",
      "  7000K .......... .......... .......... .......... .......... 44% 13.9M 1s\n",
      "  7050K .......... .......... .......... .......... .......... 44% 15.3M 1s\n",
      "  7100K .......... .......... .......... .......... .......... 45% 10.4M 1s\n",
      "  7150K .......... .......... .......... .......... .......... 45% 26.6M 1s\n",
      "  7200K .......... .......... .......... .......... .......... 45% 12.2M 1s\n",
      "  7250K .......... .......... .......... .......... .......... 46% 39.6M 1s\n",
      "  7300K .......... .......... .......... .......... .......... 46% 26.8M 1s\n",
      "  7350K .......... .......... .......... .......... .......... 46%  112M 1s\n",
      "  7400K .......... .......... .......... .......... .......... 47% 30.9M 1s\n",
      "  7450K .......... .......... .......... .......... .......... 47% 10.4M 1s\n",
      "  7500K .......... .......... .......... .......... .......... 47% 10.4M 1s\n",
      "  7550K .......... .......... .......... .......... .......... 48% 9.65M 1s\n",
      "  7600K .......... .......... .......... .......... .......... 48% 12.9M 1s\n",
      "  7650K .......... .......... .......... .......... .......... 48% 12.9M 1s\n",
      "  7700K .......... .......... .......... .......... .......... 49% 7.02M 1s\n",
      "  7750K .......... .......... .......... .......... .......... 49% 12.9M 1s\n",
      "  7800K .......... .......... .......... .......... .......... 49% 12.0M 1s\n",
      "  7850K .......... .......... .......... .......... .......... 49% 12.4M 1s\n",
      "  7900K .......... .......... .......... .......... .......... 50% 17.1M 1s\n",
      "  7950K .......... .......... .......... .......... .......... 50% 28.1M 1s\n",
      "  8000K .......... .......... .......... .......... .......... 50% 26.4M 1s\n",
      "  8050K .......... .......... .......... .......... .......... 51% 18.7M 1s\n",
      "  8100K .......... .......... .......... .......... .......... 51% 18.6M 1s\n",
      "  8150K .......... .......... .......... .......... .......... 51% 23.9M 1s\n",
      "  8200K .......... .......... .......... .......... .......... 52% 32.6M 1s\n",
      "  8250K .......... .......... .......... .......... .......... 52% 11.2M 1s\n",
      "  8300K .......... .......... .......... .......... .......... 52% 12.5M 1s\n",
      "  8350K .......... .......... .......... .......... .......... 53% 10.5M 1s\n",
      "  8400K .......... .......... .......... .......... .......... 53% 5.25M 1s\n",
      "  8450K .......... .......... .......... .......... .......... 53%  139M 1s\n",
      "  8500K .......... .......... .......... .......... .......... 54% 15.0M 1s\n",
      "  8550K .......... .......... .......... .......... .......... 54% 9.34M 1s\n",
      "  8600K .......... .......... .......... .......... .......... 54% 5.97M 1s\n",
      "  8650K .......... .......... .......... .......... .......... 55% 28.5M 1s\n",
      "  8700K .......... .......... .......... .......... .......... 55% 33.0M 1s\n",
      "  8750K .......... .......... .......... .......... .......... 55% 61.1M 1s\n",
      "  8800K .......... .......... .......... .......... .......... 55% 49.4M 1s\n",
      "  8850K .......... .......... .......... .......... .......... 56% 8.61M 1s\n",
      "  8900K .......... .......... .......... .......... .......... 56% 15.1M 1s\n",
      "  8950K .......... .......... .......... .......... .......... 56% 11.9M 1s\n",
      "  9000K .......... .......... .......... .......... .......... 57% 12.5M 1s\n",
      "  9050K .......... .......... .......... .......... .......... 57% 11.6M 1s\n",
      "  9100K .......... .......... .......... .......... .......... 57% 11.5M 1s\n",
      "  9150K .......... .......... .......... .......... .......... 58% 7.45M 1s\n",
      "  9200K .......... .......... .......... .......... .......... 58% 25.8M 1s\n",
      "  9250K .......... .......... .......... .......... .......... 58% 58.7M 1s\n",
      "  9300K .......... .......... .......... .......... .......... 59% 19.3M 1s\n",
      "  9350K .......... .......... .......... .......... .......... 59% 33.0M 1s\n",
      "  9400K .......... .......... .......... .......... .......... 59% 4.53M 1s\n",
      "  9450K .......... .......... .......... .......... .......... 60% 11.6M 1s\n",
      "  9500K .......... .......... .......... .......... .......... 60% 13.2M 1s\n",
      "  9550K .......... .......... .......... .......... .......... 60% 14.6M 1s\n",
      "  9600K .......... .......... .......... .......... .......... 61% 6.75M 1s\n",
      "  9650K .......... .......... .......... .......... .......... 61% 21.6M 1s\n",
      "  9700K .......... .......... .......... .......... .......... 61% 24.6M 1s\n",
      "  9750K .......... .......... .......... .......... .......... 62% 24.4M 1s\n",
      "  9800K .......... .......... .......... .......... .......... 62% 25.2M 1s\n",
      "  9850K .......... .......... .......... .......... .......... 62% 18.9M 1s\n",
      "  9900K .......... .......... .......... .......... .......... 62% 31.5M 1s\n",
      "  9950K .......... .......... .......... .......... .......... 63% 21.8M 1s\n",
      " 10000K .......... .......... .......... .......... .......... 63% 21.5M 1s\n",
      " 10050K .......... .......... .......... .......... .......... 63% 23.3M 1s\n",
      " 10100K .......... .......... .......... .......... .......... 64% 14.9M 1s\n",
      " 10150K .......... .......... .......... .......... .......... 64% 50.0M 1s\n",
      " 10200K .......... .......... .......... .......... .......... 64% 2.90M 1s\n",
      " 10250K .......... .......... .......... .......... .......... 65% 1.88M 1s\n",
      " 10300K .......... .......... .......... .......... .......... 65% 21.0M 1s\n",
      " 10350K .......... .......... .......... .......... .......... 65% 16.7M 1s\n",
      " 10400K .......... .......... .......... .......... .......... 66% 18.6M 1s\n",
      " 10450K .......... .......... .......... .......... .......... 66% 17.2M 1s\n",
      " 10500K .......... .......... .......... .......... .......... 66% 16.5M 1s\n",
      " 10550K .......... .......... .......... .......... .......... 67% 13.8M 1s\n",
      " 10600K .......... .......... .......... .......... .......... 67% 24.5M 1s\n",
      " 10650K .......... .......... .......... .......... .......... 67% 23.0M 1s\n",
      " 10700K .......... .......... .......... .......... .......... 68% 16.3M 1s\n",
      " 10750K .......... .......... .......... .......... .......... 68% 23.2M 1s\n",
      " 10800K .......... .......... .......... .......... .......... 68% 27.5M 0s\n",
      " 10850K .......... .......... .......... .......... .......... 68% 29.9M 0s\n",
      " 10900K .......... .......... .......... .......... .......... 69% 18.5M 0s\n",
      " 10950K .......... .......... .......... .......... .......... 69% 20.9M 0s\n",
      " 11000K .......... .......... .......... .......... .......... 69% 12.7M 0s\n",
      " 11050K .......... .......... .......... .......... .......... 70% 3.99M 0s\n",
      " 11100K .......... .......... .......... .......... .......... 70% 2.38M 0s\n",
      " 11150K .......... .......... .......... .......... .......... 70% 2.70M 0s\n",
      " 11200K .......... .......... .......... .......... .......... 71% 18.0M 0s\n",
      " 11250K .......... .......... .......... .......... .......... 71% 13.9M 0s\n",
      " 11300K .......... .......... .......... .......... .......... 71% 28.6M 0s\n",
      " 11350K .......... .......... .......... .......... .......... 72% 19.8M 0s\n",
      " 11400K .......... .......... .......... .......... .......... 72% 33.9M 0s\n",
      " 11450K .......... .......... .......... .......... .......... 72% 39.0M 0s\n",
      " 11500K .......... .......... .......... .......... .......... 73% 17.3M 0s\n",
      " 11550K .......... .......... .......... .......... .......... 73% 28.2M 0s\n",
      " 11600K .......... .......... .......... .......... .......... 73% 34.7M 0s\n",
      " 11650K .......... .......... .......... .......... .......... 74% 17.5M 0s\n",
      " 11700K .......... .......... .......... .......... .......... 74% 19.5M 0s\n",
      " 11750K .......... .......... .......... .......... .......... 74% 20.5M 0s\n",
      " 11800K .......... .......... .......... .......... .......... 74% 24.5M 0s\n",
      " 11850K .......... .......... .......... .......... .......... 75% 13.5M 0s\n",
      " 11900K .......... .......... .......... .......... .......... 75% 7.38M 0s\n",
      " 11950K .......... .......... .......... .......... .......... 75% 5.20M 0s\n",
      " 12000K .......... .......... .......... .......... .......... 76% 9.93M 0s\n",
      " 12050K .......... .......... .......... .......... .......... 76% 13.9M 0s\n",
      " 12100K .......... .......... .......... .......... .......... 76% 7.48M 0s\n",
      " 12150K .......... .......... .......... .......... .......... 77% 41.5M 0s\n",
      " 12200K .......... .......... .......... .......... .......... 77% 41.9M 0s\n",
      " 12250K .......... .......... .......... .......... .......... 77% 23.9M 0s\n",
      " 12300K .......... .......... .......... .......... .......... 78% 22.2M 0s\n",
      " 12350K .......... .......... .......... .......... .......... 78% 30.2M 0s\n",
      " 12400K .......... .......... .......... .......... .......... 78% 18.7M 0s\n",
      " 12450K .......... .......... .......... .......... .......... 79% 16.4M 0s\n",
      " 12500K .......... .......... .......... .......... .......... 79% 12.8M 0s\n",
      " 12550K .......... .......... .......... .......... .......... 79% 59.2M 0s\n",
      " 12600K .......... .......... .......... .......... .......... 80% 14.0M 0s\n",
      " 12650K .......... .......... .......... .......... .......... 80% 19.4M 0s\n",
      " 12700K .......... .......... .......... .......... .......... 80% 15.8M 0s\n",
      " 12750K .......... .......... .......... .......... .......... 80% 8.47M 0s\n",
      " 12800K .......... .......... .......... .......... .......... 81% 7.10M 0s\n",
      " 12850K .......... .......... .......... .......... .......... 81% 11.2M 0s\n",
      " 12900K .......... .......... .......... .......... .......... 81% 8.60M 0s\n",
      " 12950K .......... .......... .......... .......... .......... 82% 16.9M 0s\n",
      " 13000K .......... .......... .......... .......... .......... 82% 51.1M 0s\n",
      " 13050K .......... .......... .......... .......... .......... 82% 16.0M 0s\n",
      " 13100K .......... .......... .......... .......... .......... 83% 41.7M 0s\n",
      " 13150K .......... .......... .......... .......... .......... 83% 23.4M 0s\n",
      " 13200K .......... .......... .......... .......... .......... 83% 32.9M 0s\n",
      " 13250K .......... .......... .......... .......... .......... 84% 32.4M 0s\n",
      " 13300K .......... .......... .......... .......... .......... 84% 20.2M 0s\n",
      " 13350K .......... .......... .......... .......... .......... 84% 17.1M 0s\n",
      " 13400K .......... .......... .......... .......... .......... 85% 33.6M 0s\n",
      " 13450K .......... .......... .......... .......... .......... 85% 11.9M 0s\n",
      " 13500K .......... .......... .......... .......... .......... 85% 11.1M 0s\n",
      " 13550K .......... .......... .......... .......... .......... 86% 12.3M 0s\n",
      " 13600K .......... .......... .......... .......... .......... 86% 13.0M 0s\n",
      " 13650K .......... .......... .......... .......... .......... 86% 7.26M 0s\n",
      " 13700K .......... .......... .......... .......... .......... 86% 9.25M 0s\n",
      " 13750K .......... .......... .......... .......... .......... 87% 9.25M 0s\n",
      " 13800K .......... .......... .......... .......... .......... 87% 12.5M 0s\n",
      " 13850K .......... .......... .......... .......... .......... 87% 19.8M 0s\n",
      " 13900K .......... .......... .......... .......... .......... 88% 33.6M 0s\n",
      " 13950K .......... .......... .......... .......... .......... 88% 20.0M 0s\n",
      " 14000K .......... .......... .......... .......... .......... 88% 7.15M 0s\n",
      " 14050K .......... .......... .......... .......... .......... 89% 67.8M 0s\n",
      " 14100K .......... .......... .......... .......... .......... 89% 82.4M 0s\n",
      " 14150K .......... .......... .......... .......... .......... 89% 30.5M 0s\n",
      " 14200K .......... .......... .......... .......... .......... 90% 44.1M 0s\n",
      " 14250K .......... .......... .......... .......... .......... 90% 60.5M 0s\n",
      " 14300K .......... .......... .......... .......... .......... 90% 19.2M 0s\n",
      " 14350K .......... .......... .......... .......... .......... 91% 29.0M 0s\n",
      " 14400K .......... .......... .......... .......... .......... 91% 7.87M 0s\n",
      " 14450K .......... .......... .......... .......... .......... 91% 8.04M 0s\n",
      " 14500K .......... .......... .......... .......... .......... 92% 10.1M 0s\n",
      " 14550K .......... .......... .......... .......... .......... 92% 8.54M 0s\n",
      " 14600K .......... .......... .......... .......... .......... 92% 14.0M 0s\n",
      " 14650K .......... .......... .......... .......... .......... 93% 11.8M 0s\n",
      " 14700K .......... .......... .......... .......... .......... 93% 13.7M 0s\n",
      " 14750K .......... .......... .......... .......... .......... 93% 10.5M 0s\n",
      " 14800K .......... .......... .......... .......... .......... 93% 11.6M 0s\n",
      " 14850K .......... .......... .......... .......... .......... 94% 22.5M 0s\n",
      " 14900K .......... .......... .......... .......... .......... 94% 26.0M 0s\n",
      " 14950K .......... .......... .......... .......... .......... 94% 19.0M 0s\n",
      " 15000K .......... .......... .......... .......... .......... 95% 26.9M 0s\n",
      " 15050K .......... .......... .......... .......... .......... 95% 20.8M 0s\n",
      " 15100K .......... .......... .......... .......... .......... 95% 42.5M 0s\n",
      " 15150K .......... .......... .......... .......... .......... 96% 17.6M 0s\n",
      " 15200K .......... .......... .......... .......... .......... 96% 39.7M 0s\n",
      " 15250K .......... .......... .......... .......... .......... 96% 19.4M 0s\n",
      " 15300K .......... .......... .......... .......... .......... 97% 8.95M 0s\n",
      " 15350K .......... .......... .......... .......... .......... 97% 13.5M 0s\n",
      " 15400K .......... .......... .......... .......... .......... 97% 7.42M 0s\n",
      " 15450K .......... .......... .......... .......... .......... 98% 13.7M 0s\n",
      " 15500K .......... .......... .......... .......... .......... 98% 13.4M 0s\n",
      " 15550K .......... .......... .......... .......... .......... 98% 10.9M 0s\n",
      " 15600K .......... .......... .......... .......... .......... 99% 8.72M 0s\n",
      " 15650K .......... .......... .......... .......... .......... 99% 12.3M 0s\n",
      " 15700K .......... .......... .......... .......... .......... 99% 13.3M 0s\n",
      " 15750K .......... .......... .......... .......... .......... 99% 11.5M 0s\n",
      " 15800K .....                                                 100% 5.34M=1.5s\n",
      "\n",
      "2021-02-06 00:35:49 (10.6 MB/s) - 'mallet-2.0.8.zip.4' saved [16184794/16184794]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('mallet-2.0.8.zip', 'r') as zipObj:\n",
    "      zipObj.extractall('mallet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuMDwKR3uY0T"
   },
   "source": [
    "## Question 4: Build an LDA topic model with MALLET (1 point)\n",
    "\n",
    "__Hint:__ Refer to the tutorial and use a similar configuration for the model settings (hyperparameters). __Also set the total topics to be 20__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OcSOwOSchgRV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TOTAL_TOPICS = 20\n",
    "lda_model = gensim.models.LdaModel(corpus=bow_corpus, \n",
    "                                   id2word=dictionary, \n",
    "                                   chunksize=1740, \n",
    "                                   alpha='auto', \n",
    "                                   eta='auto', \n",
    "                                   random_state=42,\n",
    "                                   iterations=500, \n",
    "                                   num_topics=TOTAL_TOPICS, \n",
    "                                   passes=20, \n",
    "                                   eval_every=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "n5Zl1TlU3Vgo"
   },
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\xeroj\\AppData\\Local\\Temp\\c9517b_corpus.txt --output C:\\Users\\xeroj\\AppData\\Local\\Temp\\c9517b_corpus.mallet' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\wrappers\\ldamallet.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, mallet_path, corpus, num_topics, alpha, id2word, workers, prefix, optimize_interval, iterations, topic_threshold, random_seed)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_seed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfinferencer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\wrappers\\ldamallet.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         \"\"\"\n\u001b[1;32m--> 272\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmallet_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' train-topics --input %s --num-topics %s  --alpha %s --optimize-interval %s '\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[1;34m'--num-threads %s --output-state %s --output-doc-topics %s --output-topic-keys %s '\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\wrappers\\ldamallet.py\u001b[0m in \u001b[0;36mconvert_input\u001b[1;34m(self, corpus, infer, serialize_corpus)\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcmd\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfcorpustxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfcorpusmallet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"converting temporary corpus to MALLET format with %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(stdout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m   1930\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1931\u001b[0m             \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1932\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1933\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1934\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command 'mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\xeroj\\AppData\\Local\\Temp\\c9517b_corpus.txt --output C:\\Users\\xeroj\\AppData\\Local\\Temp\\c9517b_corpus.mallet' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "MALLET_PATH = 'mallet-2.0.8/bin/mallet'\n",
    "lda_mallet = gensim.models.wrappers.LdaMallet(mallet_path=MALLET_PATH, \n",
    "                                              corpus=bow_corpus, \n",
    "                                              num_topics=TOTAL_TOPICS, \n",
    "                                              id2word=dictionary,\n",
    "                                              iterations=500, \n",
    "                                              workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuQTfPlHunE3"
   },
   "source": [
    "__The model may take some time to run depending on your system config__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfnoqJZghhMP"
   },
   "source": [
    "## Question 5: View Topics (1 point)\n",
    "\n",
    "__Hint:__ The _View Topics_ section in Tutorial 1 might be useful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ex3iHUMBhjp0"
   },
   "outputs": [],
   "source": [
    "topics = [[(term, round(wt, 3)) \n",
    "               for term, wt in lda_mallet.show_topic(n, topn=20)] \n",
    "                   for n in range(0, lda_mallet.num_topics)]\n",
    "topics_df = pd.DataFrame([', '.join([term for term, wt in topic])  \n",
    "                              for topic in topics],\n",
    "                         columns = ['Terms per Topic'],\n",
    "                         index=['Topic'+str(t) for t in range(1, lda_mallet.num_topics+1)]\n",
    "                         )\n",
    "\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mK-HZ6fOhmfu"
   },
   "source": [
    "## Question 6: Evaluate Model Performance (1 point)\n",
    "\n",
    "__Note:__ print the Cv and UMass coherence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwFhvbQ6honk"
   },
   "outputs": [],
   "source": [
    "cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n",
    "                                                      texts=norm_corpus_bigrams,\n",
    "                                                      dictionary=dictionary, \n",
    "                                                      coherence='c_v')\n",
    "\n",
    "avg_coherence_cv = cv_coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHZwyaeN6ZZr"
   },
   "outputs": [],
   "source": [
    "umass_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n",
    "                                                         texts=norm_corpus_bigrams,\n",
    "                                                         dictionary=dictionary, \n",
    "                                                         coherence='u_mass')\n",
    "\n",
    "avg_coherence_umass = umass_coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gHio7177XV-"
   },
   "outputs": [],
   "source": [
    "print('Avg. Coherence Score (Cv):', avg_coherence_cv)\n",
    "print('Avg. Coherence Score (UMass):', avg_coherence_umass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hF4WlQ8V4qfY"
   },
   "source": [
    "## Inference on documents\n",
    "\n",
    "Here we will try to take some documents and predict \\ infer their topics using our trained LDA model. Do note you can use any new documents also in this scenario but you would need to transform them into relevant bag of words vectors before predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTj2l97F-PbB"
   },
   "source": [
    "#### Create a sample dataset of 3 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyyIBAq_oBCy"
   },
   "outputs": [],
   "source": [
    "sample_docs = [' '.join(doc) for doc in norm_data[5:8]]\n",
    "sample_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRErT61S-S-O"
   },
   "source": [
    "#### Check their class labels\n",
    "\n",
    "Since this is actually a labeled dataset we can see the actual class \\ category labels of these news posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JN36ZXGAoihS"
   },
   "outputs": [],
   "source": [
    "print(np.array(newsgroups_train.target_names)[newsgroups_train.target[5:8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snF0CRNI-cTJ"
   },
   "source": [
    "## Question 7: Pre-process documents (1 point)\n",
    "\n",
    "__Note:__ You can refer to Tutorial 1 or even refer to the steps above (before building them model)\n",
    "\n",
    "1. Tokenize the sample documents to get list of words per document (string splitting is useful here)\n",
    "\n",
    "2. Get bigram phrases for each tokenized document using `bigram_model`\n",
    "\n",
    "3. Use the `dictionary` built previously in the above section to get the BOW vectors using `gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIjdfQXHoxj_"
   },
   "outputs": [],
   "source": [
    "# 1. Tokenize documents\n",
    "tokenized_norm_docs = [<YOUR CODE HERE> for doc in sample_docs]\n",
    "\n",
    "# 2. Bi-gram phrases for tokenized documents\n",
    "bigram_data = [<YOUR CODE HERE> for doc in tokenized_norm_docs]\n",
    "\n",
    "# 3. BOW vectors for each document\n",
    "bow_vectorized_features = [<YOUR CODE HERE> for text in bigram_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7bCPG3r_Deg"
   },
   "source": [
    "## Question 8: Inference with trained topic model (1 point)\n",
    "\n",
    "__Note:__ Use the trained `lda_mallet` model from above to predict and get the top (most dominant) topic per document. Remember to refer to the __Interpret Results__ section in Tutorial 1 if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZUY7WaypLOy"
   },
   "outputs": [],
   "source": [
    "predicted_topics = <YOUR CODE HERE>\n",
    "top_topics = <YOUR CODE HERE>\n",
    "                     \n",
    "final_topics = [(topic+1, weight) for topic, weight in top_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLJUPY8Lpj3q"
   },
   "outputs": [],
   "source": [
    "print(final_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sld3dORTqbWN"
   },
   "outputs": [],
   "source": [
    "[topics_df.loc['Topic'+str(topic_id)]['Terms per Topic'] \n",
    "    for topic_id, weight in final_topics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hR8klT0Nm9kG"
   },
   "source": [
    "# Topic Modeling using NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeepPn1mmlzl"
   },
   "source": [
    "## Get list of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KrflzNPN7wDT"
   },
   "outputs": [],
   "source": [
    "norm_docs = [' '.join(tokenized_doc) for tokenized_doc in norm_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rCRXxSuCpZe"
   },
   "source": [
    "## Question 9: Generate Bag of Words features (1 point)\n",
    "\n",
    "__Note:__\n",
    "\n",
    "1. Use `CountVectorizer` \n",
    "2. Set `min_df` as 20 and `max_df` as 0.6\n",
    "3. Use both 1 and 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OlHqU5gkmKZ8"
   },
   "outputs": [],
   "source": [
    "cv = <YOUR CODE HERE>\n",
    "cv_features = <YOUR CODE HERE>\n",
    "\n",
    "cv_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPjVLMaUm1Sn"
   },
   "outputs": [],
   "source": [
    "vocabulary = np.array(cv.get_feature_names())\n",
    "print('Total Vocabulary Size:', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHjVQ3PbGU8y"
   },
   "source": [
    "## Question 10: Train NMF Topic Model (1 point)\n",
    "\n",
    "__Note:__ You can use a similar config as Tutorial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hujiMlutm5ay"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "nmf_model = <YOUR CODE HERE>\n",
    "document_topics = <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aNwiDogGtrC"
   },
   "source": [
    "## Question 11: Display Topics and their Terms (2 points)\n",
    "\n",
    "__Note:__ We have done a similar exercise in Tutorial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QhBWPnLm7fi"
   },
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise 1 Topic Modeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
